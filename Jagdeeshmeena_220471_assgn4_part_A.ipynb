{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagdeeshjk/GANS-text2image_Project/blob/main/Jagdeeshmeena_220471_assgn4_part_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This assignment as two parts Section A and B. The first part is the implementation of function required in applying the CNN layers and the next section will be around the use of built in function of Tensorflow"
      ],
      "metadata": {
        "id": "P3hDtc9rMxkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "`Before moving ahead as we have reached the stage of applying CNNs. We are going to deal with large number of parameters and hence more computational power. So you will need to connect runtime of collab to GPU: https://www.youtube.com/watch?v=-9CLfrZISRw`"
      ],
      "metadata": {
        "id": "48fGpBzYOjt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BOTH THE SECTION CAN BE SOLVED INDEPENDENTLY** BUT SECTION A has **3X** more weightage than SECTION B doesn't have any code to write."
      ],
      "metadata": {
        "id": "4mSFI4ExalwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SECTION:A**"
      ],
      "metadata": {
        "id": "CjXMqW4cMrFg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e8eFBZbMLxGM"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbWwF7vkLxGP"
      },
      "source": [
        "### Q1: Complete the following function corr2d(X, K), which implements the cross correlation operation for matrix X and kernel K, both are two dimensional numpy arrays (height x width). The function should return a 2 dimensional numpy array which is the result of cross correlation operation between X and K. \n",
        "\n",
        "- not giving channels right now : assume channels = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2rVZCrIDLxGT"
      },
      "outputs": [],
      "source": [
        "def corr2d(X: np.array, K: np.array) -> np.array:\n",
        "\n",
        "    # no padding for now and assume stride = 1\n",
        "    X_rows , X_cols = X.shape\n",
        "    K_rows , K_cols = K.shape\n",
        "    Y = np.zeros(( X_rows-K_rows+1 , X_cols-K_cols+1 ))\n",
        "    for i in range(Y.shape[0]):\n",
        "      for j in range(Y.shape[1]):\n",
        "        Y[i][j] = np.sum ( X [ i:i+K_rows , j:j+K_cols ] * K )\n",
        "    return Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FYF60FbvLxGd",
        "outputId": "bc48eb2a-30c4-4811-89be-9d8575e450df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[19. 25.]\n",
            " [37. 43.]]\n"
          ]
        }
      ],
      "source": [
        "X = np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]], dtype=np.float32)\n",
        "K = np.array([[0.0, 1.0], [2.0, 3.0]], dtype=np.float32)\n",
        "print(corr2d(X, K)) # example done in class, try to print this and check if you get the right answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLMP6bE3LxGf"
      },
      "source": [
        "### now try to make a new function corr2d_multiple_input_channels(X, K) : where each X and K have the same number of channels, both of them are now 3 dimensional numpy arrays, the output should be a 2 dimensional numpy array (output_h, output_w).\n",
        "\n",
        "- hint : Use the above corr2d function and read about np.stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "glmBGALxLxGg"
      },
      "outputs": [],
      "source": [
        "def corr2d_multiple_input_channels(X: np.array, K: np.array) -> np.array: \n",
        "    # write a function for this task\n",
        "      X_channels, X_rows, X_cols = X.shape\n",
        "      K_channels, K_rows, K_cols = K.shape\n",
        "    #if X_channels == K_channels:\n",
        "      Z = np.zeros((X_rows-K_rows+1 , X_cols-K_cols+1))\n",
        "      Y = np.zeros(((X_rows - K_rows + 1, X_cols - K_cols + 1)))\n",
        "      for c in range (X_channels):\n",
        "        X_channels = X[c,:,:]\n",
        "        K_channels = K[c,:,:]\n",
        "        Y = corr2d(X_channels,K_channels)\n",
        "        Z=Z+Y\n",
        "      return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Iw3vxttBLxGg",
        "outputId": "14944b03-d18b-4fa0-aaa2-d5e902c746c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_X.shape = (3, 3, 3), new_K.shape = (3, 2, 2)\n",
            "[[119. 149.]\n",
            " [209. 239.]]\n",
            "output height: 2\n",
            "output widht: 2\n"
          ]
        }
      ],
      "source": [
        "new_X = np.stack([X, X+1, X+2], axis=0) # stacking along a new dimension\n",
        "new_K = np.stack([K, K+1, K+2], axis=0) \n",
        "\n",
        "print(f\"new_X.shape = {new_X.shape}, new_K.shape = {new_K.shape}\")\n",
        "Z = corr2d_multiple_input_channels(new_X, new_K)\n",
        "print(Z)\n",
        "print(\"output height:\",Z.shape[0])\n",
        "print(\"output widht:\",Z.shape[1])\n",
        "# calculate the output by hand and then check whether you get the same answer\n",
        "# answer should be a 2 dim np array : (output_height, output_width) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhXuwIj9LxGh"
      },
      "source": [
        "### Write another function corr2d_mutli_in_out(X, K): where X (3 dim np array), K (4 dim numpy array), 0th dimension of K represents the number of kernel/filters we are using. Perform the cross correlation operation for K on X and return the output : 3 dim numpy array whose shape should be (num_output_channels, output_height, output_width)\n",
        "\n",
        "- hint : use the above corr_2d_mutliple_input_channels(X, K) for each kernel in K and then stack them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f3H-oe3xLxGi"
      },
      "outputs": [],
      "source": [
        "def corr2d_multi_in_out(X: np.array, K: np.array) -> np.array:\n",
        "    # X -> (num_in_channels, n_h, n_w)\n",
        "    # K -> (num_out_channels, num_in_channels, k_h, k_w)\n",
        "    # output -> (num_out_channels, o_h, o_w)\n",
        "    num_in_channels, X_rows, X_cols = X.shape\n",
        "    num_out_channels, num_in_channels, K_rows, K_cols = K.shape\n",
        "    output = np.zeros((num_out_channels , X_rows-K_rows+1 , X_cols-K_cols+1))\n",
        "    for i in range (K.shape[0]):\n",
        "      T = corr2d_multiple_input_channels(X , K[i,:,:,:])\n",
        "      output[i] = np.stack(T, axis=0)\n",
        "    return output\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8KvPQ4eYLxGi",
        "outputId": "b3bb3f45-eb05-4661-e1be-b353b97c854f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_K.shape = (4, 3, 2, 2)\n",
            "[[[119. 149.]\n",
            "  [209. 239.]]\n",
            "\n",
            " [[155. 197.]\n",
            "  [281. 323.]]\n",
            "\n",
            " [[191. 245.]\n",
            "  [353. 407.]]\n",
            "\n",
            " [[227. 293.]\n",
            "  [425. 491.]]]\n"
          ]
        }
      ],
      "source": [
        "my_K = np.stack([new_K, new_K+1, new_K+2, new_K + 3], axis=0) \n",
        "print(f\"my_K.shape = {my_K.shape}\")\n",
        "\n",
        "print(corr2d_multi_in_out(new_X, my_K)) # cross check the calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGjUjrXQLxGj"
      },
      "source": [
        "### Q2: What is the computational and statistical benefits of stride larger than 1?? (not more than 20 word answer for each)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_dSJFchLxGj"
      },
      "source": [
        "### Q3: Now let's implement a model with just a single convolution layer, given X(input), Y(output) and K(kernel). Y is the output of the cross-correlation operation of K on X. You need to build a model to learn that kernel K.(try to print the kernel at each epoch)\n",
        "\n",
        "- hint : conv_layer(output_channels = 1, input_channels = 1, kerenl_size=(1, 2), bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uWAF0RPhLxGj",
        "outputId": "372fb3e1-d0dd-40d7-97b4-be7f16820b84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 0. 0. 0. 0. 1. 1.]\n",
            " [1. 1. 0. 0. 0. 0. 1. 1.]\n",
            " [1. 1. 0. 0. 0. 0. 1. 1.]\n",
            " [1. 1. 0. 0. 0. 0. 1. 1.]\n",
            " [1. 1. 0. 0. 0. 0. 1. 1.]\n",
            " [1. 1. 0. 0. 0. 0. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "X = np.ones((6, 8), dtype=np.float32)\n",
        "X[:, 2:6] = 0\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pr0_V5lpLxGj",
        "outputId": "e8452851-3671-4b10-eef5-ba92eced1fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  1.  0.  0.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0. -1.  0.]]\n"
          ]
        }
      ],
      "source": [
        "K = np.array([[1.0, -1.0]], dtype=np.float32) # kernel, you need to learn this using a model\n",
        "Y = corr2d(X, K) \n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4HSVk5foLxGk"
      },
      "outputs": [],
      "source": [
        "X = X.reshape((1,6, 8,1)) # this will be the input to the model\n",
        "Y = Y.reshape((1, 6, 7,1)) # this is the output, to be used while calculation loss\n",
        "lr = 3e-2 # use this learning rate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(1,(1,2), use_bias=False))\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr), loss = 'mse', metrics = ['accuracy'])\n",
        "\n",
        "epochs = 100\n",
        "for i in range (epochs):\n",
        "  model.fit(X, Y , epochs = 1)\n",
        "  kernel = model.get_weights()[0]\n",
        "  kernel = np.array(kernel).flatten()\n",
        "  kernel = kernel.reshape((1,2))\n",
        "  print(\"Trained Kernel:\", kernel)"
      ],
      "metadata": {
        "id": "flCpasvJixdJ",
        "outputId": "1f85fa2b-3f85-45dd-8679-2e13a0d164f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 8s 8s/step - loss: 0.1234 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.6213494  -0.32799017]]\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1096 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.5927515  -0.35785502]]\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0984 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.5680295 -0.3874727]]\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0896 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.5509751 -0.4167242]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0826 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.5441225  -0.44550234]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0764 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.5466416  -0.47372994]]\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0704 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.55618834 -0.5013605 ]]\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0645 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.5707101 -0.5283691]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0586 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.5887698 -0.5547456]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0528 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.6093881 -0.5804894]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0472 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.6318725 -0.6056066]]\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0418 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.65570366 -0.6301089 ]]\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0367 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.68046623 -0.6540118 ]]\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0319 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.7058064  -0.67733353]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0275 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.7314058 -0.700094 ]]\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0234 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.75696653 -0.7223133 ]]\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0198 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.7822022 -0.7440104]]\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0166 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.8068353 -0.7652017]]\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0137 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.8305995  -0.78589934]]\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0112 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.8532455  -0.80610985]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0091 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.87454975 -0.8258324 ]]\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0073 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.89432514 -0.845058  ]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0057 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.912431   -0.86376816]]\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0044 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.928781   -0.88193464]]\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9433471  -0.89951944]]\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0024 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9561597  -0.91647536]]\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0017 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9673029 -0.9327473]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.97690666 -0.94827396]]\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.9265e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9851365  -0.96299034]]\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.6736e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9921823 -0.9768305]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5276e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9982461  -0.98973066]]\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.6223e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0035305 -1.0016326]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.1906e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0082281 -1.0124866]]\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.7127e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0125115 -1.0222546]]\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2024e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0165259 -1.0309123]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3466e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0203825 -1.0384511]]\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 3.6384e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0241556 -1.0448786]]\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.9378e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0278809 -1.0502193]]\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.1390e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0315572 -1.0545131]]\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.1735e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0351497 -1.0578141]]\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.0076e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0385958 -1.0601885]]\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 8.6354e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0418121 -1.0617118]]\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.0694e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0447024 -1.0624658]]\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.3305e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0471667 -1.0625359]]\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.4398e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0491098 -1.0620079]]\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9.4135e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0504493 -1.0609654]]\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 9.2616e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0511223 -1.0594872]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.9888e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0510914 -1.0576457]]\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.5990e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0503471 -1.0555053]]\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.0984e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0489092 -1.0531219]]\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.4993e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.046826  -1.0505427]]\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.8212e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0441713 -1.0478061]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.0899e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0410398 -1.0449431]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.3352e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0375414 -1.0419782]]\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.5870e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0337949 -1.0389311]]\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.8721e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0299215 -1.0358181]]\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 3.2111e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0260382 -1.0326539]]\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.6169e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0222514 -1.029453 ]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.0948e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0186524 -1.0262309]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6441e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0153133 -1.0230054]]\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2601e-04 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0122844 -1.0197964]]\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.3666e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0095937 -1.0166268]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.6774e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0072476 -1.0135221]]\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.4874e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0052329 -1.0105095]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7645e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0035204 -1.007617 ]]\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.4854e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0020689 -1.0048727]]\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 6.2494e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0008302 -1.002303 ]]\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.4759e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9997537  -0.99993205]]\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8412e-08 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9987911 -0.9977799]]\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2050e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99790007 -0.99586207]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.2627e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99704754 -0.99418855]]\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.4054e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9962117  -0.99276364]]\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.2928e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9953828 -0.9915858]]\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.7279e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99456275 -0.990648  ]]\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1096e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9937639  -0.98993826]]\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.4200e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9930069 -0.9894406]]\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.6549e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9923176  -0.98913604]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8184e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9917245 -0.9890035]]\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.9174e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99125487 -0.9890211 ]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.9570e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99093235 -0.9891674 ]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.9400e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9907743  -0.98942184]]\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8667e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99079    -0.98976576]]\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7380e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9909799 -0.9901828]]\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5573e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9913351  -0.99065924]]\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3321e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9918383  -0.99118376]]\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 2.0742e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9924651 -0.9917474]]\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.7987e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9931859 -0.992343 ]]\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.5212e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99396807 -0.9929651 ]]\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2555e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9947786 -0.9936089]]\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0121e-05 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99558604 -0.99426997]]\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 7.9686e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99636275 -0.9949438 ]]\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 6.1174e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99708647 -0.9956253 ]]\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 4.5566e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9977413  -0.99630874]]\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 3.2616e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9983181 -0.9969874]]\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.2066e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9988143 -0.9976539]]\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.3719e-06 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9992333 -0.9983004]]\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.4532e-07 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.99958324 -0.9989186 ]]\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.1809e-07 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 0.9998755 -0.9995006]]\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 7.8007e-08 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0001231 -1.000039 ]]\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.4072e-09 - accuracy: 0.8571\n",
            "Trained Kernel: [[ 1.0003393 -1.0005273]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sWbsvXQLxGk"
      },
      "source": [
        "### Q4: Complete the following function max_pool2d(X, K) which performs maxpooling with kernel size K on X and returns and two dim numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0Qjsv_v_LxGk"
      },
      "outputs": [],
      "source": [
        "def max_pool2d(X: np.array, K: tuple) -> np.array:\n",
        "    # write a function for this job\n",
        "    X_rows , X_cols = X.shape\n",
        "    K_rows , K_cols = K\n",
        "    Y = np.zeros(( X_rows-K_rows+1 , X_cols-K_cols+1 ))\n",
        "    for i in range(Y.shape[0]):\n",
        "      for j in range(Y.shape[1]):\n",
        "        Y[i][j] = np.max( X[i : i + K_rows ,j : j + K_cols ])\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZiLo_mjSLxGl",
        "outputId": "18a22150-b9b4-4ea6-8de9-faf00ede081b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4., 5.],\n",
              "       [7., 8.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X = np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]], dtype=np.float32)\n",
        "max_pool2d(X, (2, 2)) # check if the output matches with your calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-niSYY3LxGl"
      },
      "source": [
        "# **SECTION:B**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **JUST RUN THE CELLS AND VISUALIZE**( Nothing to code 🙂 )\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "pfa0otIGY4vf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI1blAXSLxGl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the dataset\n",
        "(X_train,Y_train),(X_test,Y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X_train,Y_train,test_size=0.3)"
      ],
      "metadata": {
        "id": "hjSRQ2QuNf69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shape of all 4 variables: X_train,Y_train,X_test, and Y_test\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "id": "fhl64KENQSuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets visualize the CIFAR-10 dataset\n",
        "\n",
        "import random\n",
        "figure = plt.figure(figsize=(6,6))\n",
        "\n",
        "for i in range(9):\n",
        "  index = random.randint(0,len(X_train)-1) # showing the index_th image\n",
        "  \n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.imshow(X_train[index])\n",
        "  plt.title(Y_train[index])\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "Oygw7B1fNm0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "So you can probably notice here that the images are 3D(coloured) but still \n",
        "not of great quality ( what can you expect from 32x32 image). Also there are\n",
        "certain other factors which makes the classification a bit tougher than the \n",
        "cases of 2D( the digit and the fashion data) you dealt before. We will try to\n",
        "understand the difficulties and find probable solution for them.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mIY0GmLjSMvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "ATuu2w_SNuCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Lets first start with the CNN model discussed in the class for digit\n",
        " classification. Notice that I have changed the input shape for this usecase.\n",
        " Earlier it was (28,28,1) for the digit dataset.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w1yCGBaMSz5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model initialization\n",
        "cnn_model = tf.keras.Sequential()\n",
        "\n",
        "# adding the 1st layer of CNN\n",
        "cnn_model.add(tf.keras.layers.Conv2D(26, (5,5), activation = 'relu', input_shape=(32,32,3)))\n",
        "\n",
        "# adding a maxpooling\n",
        "cnn_model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "\n",
        "#adding another CNN layer\n",
        "cnn_model.add(tf.keras.layers.Conv2D(16, (5,5), activation = 'relu'))\n",
        "\n",
        "# adding another maxpooling layer\n",
        "cnn_model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "\n",
        "#flattening the layer\n",
        "cnn_model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# 20 x 20 x 16\n",
        "#dense layer\n",
        "cnn_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "\n",
        "# final layer \n",
        "cnn_model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "UNr0XFiKNpnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "tWoQIGv7N0tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Explain what is Adam optimizer below in atleast 250 words.[ read on web and explain ]\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yau4nndbTU4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Make sure that you are connected to GPU runtime other wise the training in next cell is going to take a long time**"
      ],
      "metadata": {
        "id": "x5rLNjUlPKrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = cnn_model.fit(X_train, Y_train, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "id": "9RPCnjevN3W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Write about validation accuracy in 100 words.\n",
        "\n",
        "Answer: \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XMqr0nejTrrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'],label=\"Train accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], label = \"Validation accuracy\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "2AkZcRmbN6SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "What do you think is happening? Is the model training or not?\n",
        " You can see that both the training and validation accuracy are\n",
        " just roaming around 0.1. \n",
        " One reason for this can be our model architecture. We had 26 filters\n",
        " in our first layer and 16 filters in our next layer. This funnel down approach\n",
        " works for dense layers but for Conv layers( which are good at feature extraction)\n",
        " we want them to extract more and more features.\n",
        "\n",
        " So lets change that to funne up --> 16 and 32 in the layers respectively\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xEQ0UpBDTN_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model initialization\n",
        "cnn_model = tf.keras.Sequential()\n",
        "\n",
        "\"\"\"adding the 1st layer of CNN(Changed)\"\"\"\n",
        "cnn_model.add(tf.keras.layers.Conv2D(16, (5,5), activation = 'relu', input_shape=(32,32,3)))\n",
        "\n",
        "# adding a maxpooling\n",
        "cnn_model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "\n",
        "\"\"\"adding the 2nd layer of CNN(Changed)\"\"\"\n",
        "cnn_model.add(tf.keras.layers.Conv2D(32, (5,5), activation = 'relu'))\n",
        "\n",
        "# adding another maxpooling layer\n",
        "cnn_model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "\n",
        "#flattening the layer\n",
        "cnn_model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# 20 x 20 x 16\n",
        "#dense layer\n",
        "cnn_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "\n",
        "# final layer \n",
        "cnn_model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "6HcpdtBuN8_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "tFXQBRGPRjK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = cnn_model.fit(X_train, Y_train, epochs=20, validation_split=0.2)"
      ],
      "metadata": {
        "id": "WzC0iaowRmpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'],label=\"Train accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], label = \"Validation accuracy\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "HdnpcDQoRrGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Did our model improved??\n",
        "Are we done with the job?\n",
        "What is happening after 5( roughly ) epochs? Why is there a gap between\n",
        "training and validation accuracy?\n",
        "\n",
        "Our training accuracy reached to 0.73( and still increasing ) but \n",
        "the validation accuracy seems to stagnate at 0.52.\n",
        "\n",
        "Is our model overfitting on the training data so much that it can't work well \n",
        "on unseen data.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MX4njN_DUwlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "How are we gonna tackel this problem?\n",
        "Well, we will see that in next part of this assignment.\n",
        "Till then lets learn about about overfitting.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3l82eHENWI-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Well using Chatgpt to answer these question is something which everyone can think of right?? To create a difference write answer in your own terms after reading from web or reading the answer of Chatgpt.**\n",
        "### **After all this is your midterm evaluation. Cheating is something we can catch easily** ( we have also done this )"
      ],
      "metadata": {
        "id": "m5PHy4o-XSpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# please take note of this change: \n",
        "**[For all the subjective questions after this question( in the ss ) you need put the link of articles that you referred while searching about the question.]**\\\n",
        "Ideally you should refer to at least 2 articles for each of those questions.\\\n",
        "Also you do not need to go into deep maths of those( a qualitative answer is what we will be looking for)"
      ],
      "metadata": {
        "id": "mFJ-Or0zsHUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[CLARIFICATION]: Reading and understanding by searching on Chatgpt is not \n",
        "considered as cheating as long as you are writing that in your own word\n",
        "( only problem is the crediblity of its information)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7AzClJq2Z8zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question: What is overfitting and underfitting below ( atleast 200 words )\n",
        "\n",
        "Answer: \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aNTS7DqAWXVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question: What are regularization techniques in machine learning?(200 words)\n",
        "\n",
        "Answer: \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TfDCiJG1X652"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question: What dropout layer and what does it do?( read it from Tensorflow.org and write in 200 words)\n",
        "\n",
        "Answer: \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_VGMmK3XYEcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question: What is L1 normalization? write its formulae as well( atleast 200 words )\n",
        "\n",
        "Answer: \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xxRFxeCzYqir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question: What is L2 normalization? write its formulae as well( atleast 200 words )\n",
        "\n",
        "Answer: \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Qdqqsw9iYyMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question: What is data augmentation techniques and why is it needed in machine learning?( atleast 200 words )\n",
        "\n",
        "Answer: \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "E-U7uWKxY0yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Did you take note of the change in the cell at the start of these subjective questions ?**"
      ],
      "metadata": {
        "id": "Om6SQ7EBu0G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"ANS(Yes/No):       \""
      ],
      "metadata": {
        "id": "5CRLaL2UvKNW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}